# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11lhUS22W7BcaCZQ_m_Y1JMOOqwEDynhf
"""

# Replace all model definitions with this single one at the top
model_name = "microsoft/phi-2"  # Better for CPU than TinyLlama

# Import the necessary class from the transformers library
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # Import the torch library

# Change model loading to:
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu",
    torch_dtype=torch.float32  # Changed from float16
)

# It's generally good practice to load the tokenizer as well
tokenizer = AutoTokenizer.from_pretrained(model_name)

def process_query(pdf_file, question):
    try:
        if not pdf_file:
            return "Please upload a PDF document first."

        # Test PDF reading first
        try:
            text = extract_text_from_pdf(pdf_file)
            if len(text) < 10:  # Check if text extraction worked
                return "Error: Could not extract text from PDF (might be image-based)"
        except Exception as e:
            return f"PDF reading error: {str(e)}"

        text = clean_text(text)
        chunks = chunk_text(text)

        # Add progress feedback
        yield "Processing PDF... (step 1/4)"

        index, embeddings = create_vector_store(chunks)
        yield "Analyzing content... (step 2/4)"

        relevant_chunks = find_relevant_chunks(question, index, chunks, embeddings)
        yield "Generating answer... (step 3/4)"

        answer = generate_answer(question, "\n\n".join(relevant_chunks[:2]))
        yield "Formatting results... (step 4/4)"

        return f"**Answer:** {answer}\n\n**Relevant excerpts:**\n- " + "\n- ".join([c[:150]+"..." for c in relevant_chunks[:2]])

    except Exception as e:
        return f"Error: {str(e)}"

import gradio as gr

with gr.Blocks(theme=gr.themes.Soft()) as app:
    gr.Markdown("# ðŸ“š StudyMate (Optimized CPU Version)")
    with gr.Row():
        pdf_input = gr.File(label="Upload PDF", type="binary")
        question_input = gr.Textbox(label="Your Question", placeholder="Ask about the PDF content...")
    submit_btn = gr.Button("Get Answer", variant="primary")
    output = gr.Markdown()
    status = gr.Textbox(label="Status", interactive=False)

    submit_btn.click(
        process_query,
        [pdf_input, question_input],
        [output, status]
    )

app.launch(share=True, debug=True)

import psutil
print(f"Available RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB")

# Add this test right after your imports
test_text = extract_text_from_pdf("test.pdf")  # Use a known good PDF
print(f"Extracted {len(test_text)} characters")

# Test the model without PDF processing
test_prompt = "What is the capital of France?"
print(generate_answer(test_prompt, "France is a country in Europe."))